{"cells":[{"cell_type":"markdown","metadata":{},"source":["In this notebook, I plan on learning more about object detection by going over the following notebooks below. Most of the code is taken from the notebooks below except for some minor changes - all credits go to the original authors.\n","\n","Relevant notebooks:\n","\n","- https://www.kaggle.com/code/pestipeti/pytorch-starter-fasterrcnn-train\n","- https://www.kaggle.com/code/nvnnghia/fasterrcnn-pseudo-labeling"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["import os\n","import re\n","import cv2\n","import time\n","import numpy as np\n","import pandas as pd\n","from PIL import Image\n","from tqdm import tqdm\n","import albumentations as A\n","from albumentations.pytorch.transforms import ToTensorV2\n","\n","import torch\n","import torchvision\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.cuda.amp as amp\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, Dataset\n","from torch.utils.data.sampler import SequentialSampler\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection import FasterRCNN\n","from torchvision.models.detection.rpn import AnchorGenerator\n","\n","from matplotlib import pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["cfg = {\n","    \"DIR_INPUT\": '/kaggle/input/global-wheat-detection',\n","    \"DIR_TRAIN\": f'/kaggle/input/global-wheat-detection/train',\n","    \"DIR_TEST\": f'/kaggle/input/global-wheat-detection/test',\n","    \"num_epochs\": 5,\n","    'use_amp': True,\n","    'model_file': 'best_loss_min.pth',\n","    'detection_threshold': 0.5,\n","    'log_dir': './logs/'\n","}\n","\n","os.makedirs(cfg['log_dir'], exist_ok=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def load_data(cfg):\n","    train_df = pd.read_csv(f'{cfg[\"DIR_INPUT\"]}/train.csv')\n","    bboxs = np.stack(train_df['bbox'].apply(lambda x: np.fromstring(x[1:-1], sep=',')))\n","    for i, column in enumerate(['x', 'y', 'w', 'h']):\n","        train_df[column] = bboxs[:,i]\n","    train_df.drop(columns=['bbox'], inplace=True)\n","    return train_df"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_df = load_data(cfg)\n","train_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.model_selection import KFold, StratifiedKFold\n","\n","kf = KFold(5)\n","train_df['fold'] = -1\n","for fold, (train_idx, valid_idx) in enumerate(kf.split(train_df, train_df)):\n","    train_df.loc[valid_idx, 'fold'] = fold\n","\n","train_df.tail()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class WheatDataset(Dataset):\n","    def __init__(self, df, image_dir, transforms=None):\n","        super().__init__()\n","        \n","        self.image_ids = df['image_id'].unique()\n","        self.df = df\n","        self.image_dir = image_dir\n","        self.transforms = transforms\n","    \n","    def __len__(self):\n","        return self.image_ids.shape[0]\n","\n","    def __getitem__(self, index):\n","        image_id = self.image_ids[index]\n","        records = self.df[self.df['image_id'] == image_id]\n","        \n","        image = cv2.imread(f'{self.image_dir}/{image_id}.jpg', cv2.IMREAD_COLOR)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n","        image /= 255.0\n","\n","        boxes = records[['x', 'y', 'w', 'h']].values\n","        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n","        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n","        \n","        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n","        area = torch.as_tensor(area, dtype=torch.float32)\n","        \n","        # there is only one class\n","        labels = torch.ones((records.shape[0],), dtype=torch.int64)\n","        \n","        # suppose all instances are not crowd\n","        iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)\n","        \n","        target = {}\n","        target['boxes'] = boxes\n","        target['labels'] = labels\n","        # target['masks'] = None\n","        target['image_id'] = torch.tensor([index])\n","        target['area'] = area\n","        target['iscrowd'] = iscrowd\n","\n","        if self.transforms:\n","            sample = {\n","                'image': image,\n","                'bboxes': target['boxes'],\n","                'labels': labels\n","            }\n","            sample = self.transforms(**sample)\n","            image = sample['image']\n","            \n","            target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n","\n","        return image, target, image_id"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def get_train_transform():\n","    return A.Compose([\n","        A.Flip(0.5),\n","        ToTensorV2(p=1.0)\n","    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n","\n","def get_valid_transform():\n","    return A.Compose([\n","        ToTensorV2(p=1.0)\n","    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n","\n","def collate_fn(batch):\n","        return tuple(zip(*batch))\n","    \n","def train_func(model, loader_train, optimizer, scaler, device):\n","    model.train()\n","    \n","    train_loss = []\n","    bar = tqdm(loader_train)\n","    for images, targets, image_ids in bar:\n","        optimizer.zero_grad()\n","        images = list(image.to(device) for image in images)\n","        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","        \n","        with amp.autocast():\n","            loss_dict = model(images, targets)\n","            loss = sum(loss for loss in loss_dict.values())\n","    \n","        train_loss.append(loss.item())\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","        \n","        bar.set_description(f'smth:{np.mean(train_loss[-30:]):.4f}')\n","    return np.mean(train_loss)\n","\n","# def valid_func(model, loader_valid, device):\n","#     model.eval()\n","    \n","#     result = []\n","#     testdf_psuedo = []\n","#     bar = tqdm(loader_valid)\n","#     with torch.no_grad():\n","#         for images, targets, image_ids in bar:\n","#             images = list(image.to(device) for image in images)\n","#             targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","            \n","#             outputs = model(images)\n","            \n","#             for i, image in enumerate(images):\n","                \n","#                 boxes = outputs[i]['boxes'].data.cpu().numpy()\n","#                 scores = outputs[i]['scores'].data.cpu().numpy()\n","\n","#                 boxes = boxes[scores >= cfg['detection_threshold']].astype(np.int32)\n","#                 scores = scores[scores >= cfg['detection_threshold']]\n","#                 image_id = image_ids[i]\n","\n","#                 boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n","#                 boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n","\n","#                 for box in boxes:\n","#                     #print(box)\n","#                     result = {\n","#                         'image_id': 'nvnn'+image_id,\n","#                         'width': 1024,\n","#                         'height': 1024,\n","#                         'source': 'nvnn',\n","#                         'x': box[0],\n","#                         'y': box[1],\n","#                         'w': box[2],\n","#                         'h': box[3]\n","#                     }\n","                    \n","#                     testdf_psuedo.append(result)\n","#     return testdf_pseudo\n","\n","        \n","def run(fold):\n","    log_file = os.path.join(cfg['log_dir'], f'log.txt')\n","    \n","    train_ = train_df[train_df['fold'] != fold].reset_index(drop=True)\n","    valid_ = train_df[train_df['fold'] == fold].reset_index(drop=True)\n","    dataset_train = WheatDataset(train_, cfg['DIR_TRAIN'], get_train_transform())\n","    dataset_valid = WheatDataset(valid_, cfg['DIR_TRAIN'], get_valid_transform())\n","    \n","    loader_train = DataLoader(\n","        dataset_train,\n","        batch_size=16,\n","        shuffle=False,\n","        num_workers=4,\n","        collate_fn=collate_fn\n","    )\n","\n","    loader_valid = DataLoader(\n","        dataset_valid,\n","        batch_size=8,\n","        shuffle=False,\n","        num_workers=4,\n","        collate_fn=collate_fn\n","    )\n","    \n","    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","    num_classes = 2  # 1 class (wheat) + background\n","    # get number of input features for the classifier\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","    # replace the pre-trained head with a new one\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","    model.to(device)\n","    \n","    params = [p for p in model.parameters() if p.requires_grad]\n","    optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n","    scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, cfg['num_epochs'])\n","    scaler = torch.cuda.amp.GradScaler() if cfg['use_amp'] else None\n","    \n","    loss_min = np.inf\n","    \n","    for epoch in range(1, cfg['num_epochs']+1):\n","        scheduler_cosine.step(epoch-1)\n","        \n","        train_loss = train_func(model, loader_train, optimizer, scaler, device)\n","        # valid_loss = valid_func(model, loader_valid, device)\n","        \n","        content = time.ctime() + ' ' + f'Fold {fold}, Epoch {epoch}, lr: {optimizer.param_groups[0][\"lr\"]:.7f}, train loss: {train_loss:.5f}.'\n","        print(content)\n","        with open(log_file, 'a') as appender:\n","            appender.write(content + '\\n')\n","        \n","        if train_loss < loss_min:\n","            print(f'loss_min ({loss_min:.6f} --> {train_loss:.6f}). Saving model ...')\n","            torch.save(model.state_dict(), cfg['model_file'])\n","            loss_min = train_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["if __name__ == \"__main__\":\n","    run(0)\n","    run(1)\n","    run(2)\n","    run(3)\n","    run(4)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
